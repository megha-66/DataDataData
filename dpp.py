# -*- coding: utf-8 -*-
"""DPP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x0xx1ETvTx2toeC4GoUVPhyvaLPaupQ3
"""

# @title EXPLORATORY DATA ANALYSIS
import pandas as pd
import numpy as np
data = pd.read_csv('/content/sloganlist.csv')
df = pd.DataFrame(data)
#printing the dataset
df

# basic information about the dataset
df.info()

# Checking for null values in the dataset
df.isnull().sum()

# Finding the frequency of word in the 'Company' attribute
import matplotlib.pyplot as plt
from wordcloud import WordCloud
company_wordcloud = WordCloud(width=800, height=400, max_words=200, background_color='white').generate(' '.join(data['Company']))
plt.figure(figsize=(10, 5))
plt.imshow(company_wordcloud, interpolation='bilinear')
plt.title('Word Cloud for Company Names')
plt.axis('off')
plt.show()

# Finding the frequency of each word in the 'Slogan' attribute
import matplotlib.pyplot as plt
from wordcloud import WordCloud

slogan_wordcloud = WordCloud(width=800, height=400, max_words=200, background_color='white').generate(' '.join(data['Slogan']))
plt.figure(figsize=(10, 5))
plt.imshow(slogan_wordcloud, interpolation='bilinear')
plt.title('Word Cloud for Slogans')
plt.axis('off')
plt.show()

df.head(5)

# @title DATA CLEANING

#removing redundant values
import pandas as pd
data = pd.read_csv('/content/sloganlist.csv')
df = pd.DataFrame(data)
df1 = df.drop_duplicates()
df1.to_csv('data_not_redundant.csv')
df2 = pd.DataFrame(df)
df2

# Calculating the % of data reduced after removing redundancy
percentage_reduced = (( len(df) - len(df1)) / len(df)) * 100
print(f"Percentage of data reduced: {percentage_reduced:.2f}%")

# @title Text Cleaning
# Ensuring Data Consistency
col1 = 'Company'
col2 = 'Slogan'
df[col1] = df[col1].str.upper()  # turning all text to uppercase in Company attribute
df[col2] = df[col2].str.lower()   # turning all text to lowercase in Slogan attribute
df

# @title Number of Slogans per Company

import matplotlib.pyplot as plt

# Assuming 'df' is your DataFrame
slogans_per_company = df['Company'].value_counts()

plt.figure(figsize=(10, 6))
plt.hist(slogans_per_company, bins=20, edgecolor='black')
plt.xlabel('Number of Slogans')
plt.ylabel('Number of Companies')
_ = plt.title('Distribution of Slogans per Company')

# @title DATA ENCODING
# One Hot Encoding
cols = [ 'Company', 'Slogan']
enc = pd.get_dummies(df, columns = cols)
enc.to_csv('newds.csv',index = False)
df2 = pd.DataFrame(enc)
df2

# @title SENTIMENT ANALYSIS OF TEXTUAL 'slogan' Attribute
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import pandas as pd

data = pd.read_csv('/content/data_not_redundant.csv')
df1 = pd.DataFrame(data)
nltk.download('vader_lexicon')

# Initialize the VADER sentiment analyzer
sia = SentimentIntensityAnalyzer()

# Create an empty list to store sentiment results
sentiments = []

# Loop through slogans and analyze sentiment
for slogan in df1['Slogan']:
    # Ensure the slogan is a string
    if not isinstance(slogan, str):
        slogan = str(slogan)

    sentiment_scores = sia.polarity_scores(slogan)

    # Determine sentiment based on the compound score
    compound_score = sentiment_scores['compound']
    if compound_score >= 0.05:
        sentiment = 'Positive'
    elif compound_score <= -0.05:
        sentiment = 'Negative'
    else:
        sentiment = 'Neutral'

    # Store the sentiment result
    sentiments.append(sentiment)

# Add the sentiment results to the DataFrame
df1['Sentiment'] = sentiments

# Display the DataFrame with sentiments
df1.to_csv('sentiment_based_data.csv')
df3 = pd.DataFrame(df1)
df3

import pandas as pd
import matplotlib.pyplot as plt
sentiment_counts = df3['Sentiment'].value_counts()

# Plot the sentiment distribution using a pie chart
plt.figure(figsize=(6, 6))
plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Sentiment Distribution')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Show the pie chart
plt.show()

# @title DATA VISUALIZATION
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

corpus = df['Company'] + ' ' + df['Slogan']

# Combine all text into a single string
text = ' '.join(corpus)

# Create a word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Company-Slogan Pairs')
plt.show()

import matplotlib.pyplot as plt
num_companies_to_keep = 25
# Randomly select companies to keep separately
unique_companies = df['Company'].unique()
companies_to_keep = unique_companies[:num_companies_to_keep]

# Group the rest of the companies as 'Others'
df['Company'] = df['Company'].apply(lambda x: x if x in companies_to_keep else 'Others')

# Create a bar graph to show the distribution of slogans across companies
company_counts = df['Company'].value_counts()
plt.figure(figsize=(10, 5))
plt.bar(company_counts.index, company_counts.values)
plt.xlabel('Company')
plt.ylabel('Number of Slogans')
plt.title('Distribution of Slogans Across Companies')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

slogan_lengths = df['Slogan'].str.len()
plt.figure(figsize=(8, 5))
plt.hist(slogan_lengths, bins=20, edgecolor='k')
plt.xlabel('Slogan Length')
plt.ylabel('Frequency')
plt.title('Histogram of Slogan Lengths')
plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Example dataset (replace this with your dataset)
data = pd.read_csv('/content/sloganlist.csv')
# Create a DataFrame from your dataset
df = pd.DataFrame(data)

pivot_df = df.pivot_table(index='Company', columns='Slogan', aggfunc=lambda x: 1, fill_value=0)

# Create the heatmap using Seaborn
plt.figure(figsize=(50, 6))  # Adjust the figure size as needed
sns.heatmap(pivot_df, annot=True, cmap='YlGnBu', fmt='g', linewidths=.5)

# Customize labels and titles
plt.xlabel('Slogan')
plt.ylabel('Company')
plt.title('Company-Slogan Heatmap')

# Show the plot
plt.show()

!pip install spacy
!python -m spacy download en_core_web_md

import spacy
import pandas as pd

# Load the pre-trained spaCy model with word embeddings
nlp = spacy.load("en_core_web_md")

data = pd.read_csv('/content/sloganlist.csv')

df = pd.DataFrame(data)

# Function to generate word embeddings for a given text
def get_word_embeddings(text):
    doc = nlp(text)
    # Get the average word embedding for the entire text
    return doc.vector

# Apply the get_word_embeddings function to your dataset
df['WordEmbeddings'] = df['Slogan'].apply(get_word_embeddings)

# Display the DataFrame with word embeddings
df.to_csv('wordemb.csv')
df

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity

# Example dataset (replace this with your dataset)
data = pd.read_csv('/content/wordemb.csv')

# Create a DataFrame from your dataset
df3 = pd.DataFrame(data)

# Calculate cosine similarity between word embeddings
word_embeddings_matrix = np.vstack(df['WordEmbeddings'])
similarity_matrix = cosine_similarity(word_embeddings_matrix)

# Create a heatmap to visualize the similarity
plt.figure(figsize=(8, 6))
sns.heatmap(similarity_matrix, annot=True, xticklabels=df['Slogan'], yticklabels=df['Slogan'], cmap='YlGnBu')
plt.title('Cosine Similarity between Slogans based on Word Embeddings')
plt.show()